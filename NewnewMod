import json
import logging
import requests
from redis import Redis
import pandas as pd
from typing import List, Optional, Union

class DataFetcher:
    """Class to handle data fetching from Tableau Server."""
    def __init__(self, server_url: str, api_token: str):
        self.server_url = server_url
        self.api_token = api_token
        self.logger = logging.getLogger(__name__)

    def fetch_data(self) -> List[pd.DataFrame]:
        """Fetch data from Tableau Server and return as a list of DataFrames."""
        try:
            headers = {'Authorization': f'Bearer {self.api_token}'}
            response = requests.get(f'{self.server_url}/api/data', headers=headers, timeout=600)
            response.raise_for_status()
            data = response.json()
            # Simulate splitting into 3 DataFrames (adjust based on actual data structure)
            # For this example, I assume data is a list or dict that can be split
            if isinstance(data, list):
                # Split into 3 roughly equal parts (example logic)
                chunk_size = len(data) // 3 or 1
                df1 = pd.DataFrame(data[:chunk_size])
                df2 = pd.DataFrame(data[chunk_size:chunk_size*2])
                df3 = pd.DataFrame(data[chunk_size*2:])
            else:
                # Fallback: create 3 identical DataFrames if data isn't splittable
                df = pd.DataFrame(data)
                df1, df2, df3 = df, df, df
            
            self.logger.info("Successfully fetched and split data into 3 DataFrames from Tableau Server.")
            return [df1, df2, df3]
        except Exception as e:
            self.logger.error(f"Failed to fetch data: {str(e)}")
            raise

class DataCache:
    """Class to manage caching with Redis."""
    def __init__(self, redis_client: Redis):
        self.redis = redis_client
        self.logger = logging.getLogger(__name__)

    def set_data(self, key: str, df: pd.DataFrame, ttl: int = 3600) -> None:
        """Cache a single DataFrame as JSON string with an optional TTL."""
        try:
            value = df.to_json(orient='records')
            self.redis.setex(key, ttl, value)
            self.logger.info(f"Cached data with key: {key}")
        except Exception as e:
            self.logger.error(f"Failed to cache data: {str(e)}")
            raise

    def set_data_list(self, base_key: str, df_list: List[pd.DataFrame], ttl: int = 3600) -> None:
        """Cache a list of DataFrames with numbered keys (e.g., base_key_0, base_key_1)."""
        try:
            for i, df in enumerate(df_list):
                key = f"{base_key}_{i}"
                self.set_data(key, df, ttl)
            self.logger.info(f"Cached {len(df_list)} DataFrames with base key: {base_key}")
        except Exception as e:
            self.logger.error(f"Failed to cache DataFrame list: {str(e)}")
            raise

    def get_data(self, key: str) -> Optional[pd.DataFrame]:
        """Retrieve a single DataFrame from cache."""
        try:
            data = self.redis.get(key)
            if data:
                return pd.read_json(data, orient='records')
            return None
        except Exception as e:
            self.logger.error(f"Failed to retrieve cached data: {str(e)}")
            return None

    def get_data_list(self, base_key: str, num_dfs: int = 3) -> List[Optional[pd.DataFrame]]:
        """Retrieve a list of DataFrames from cache."""
        try:
            df_list = []
            for i in range(num_dfs):
                key = f"{base_key}_{i}"
                df = self.get_data(key)
                df_list.append(df)
            return df_list
        except Exception as e:
            self.logger.error(f"Failed to retrieve cached DataFrame list: {str(e)}")
            return [None] * num_dfs

class MetricComputer:
    """Class to compute metrics from DataFrame data."""
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.logger = logging.getLogger(__name__)

    def compute(self, metric_name: str) -> Union[float, int, None]:
        """Compute a specific metric from the DataFrame."""
        try:
            if self.df.empty:
                raise ValueError("No data available for computation")
            
            if metric_name == "total_count":
                return len(self.df)
            elif metric_name == "avg_value":
                if "value" not in self.df.columns:
                    raise ValueError("'value' column not found in DataFrame")
                return self.df["value"].mean()
            else:
                self.logger.warning(f"Unknown metric: {metric_name}")
                return None
                
            self.logger.info(f"Computed metric: {metric_name}")
        except Exception as e:
            self.logger.error(f"Error computing metric {metric_name}: {str(e)}")
            return None

# Example usage
if __name__ == "__main__":
    # Configure logging
    logging.basicConfig(level=logging.INFO)
    
    # Initialize components
    fetcher = DataFetcher("https://tableau.example.com", "your-api-token")
    redis_client = Redis(host='localhost', port=6379, db=0)
    cache = DataCache(redis_client)
    
    # Fetch and cache data
    try:
        df_list = fetcher.fetch_data()
        if len(df_list) != 3:
            raise ValueError(f"Expected 3 DataFrames, got {len(df_list)}")
        
        # Cache the list of DataFrames
        cache.set_data_list("tableau_data", df_list)
        
        # Retrieve from cache and compute metrics
        cached_df_list = cache.get_data_list("tableau_data", num_dfs=3)
        for i, df in enumerate(cached_df_list):
            if df is not None:
                computer = MetricComputer(df)
                total = computer.compute("total_count")
                avg = computer.compute("avg_value")
                print(f"DataFrame {i}: Total Count: {total}, Average Value: {avg}")
            else:
                print(f"DataFrame {i}: Failed to retrieve from cache")
    except Exception as e:
        logging.error(f"Main execution failed: {str(e)}")
